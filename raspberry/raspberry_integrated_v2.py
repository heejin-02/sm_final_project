#!/usr/bin/env python3
"""
ë¼ì¦ˆë² ë¦¬íŒŒì´ í†µí•© í•´ì¶© íƒì§€ ì‹œìŠ¤í…œ V2
Spring Boot ì—°ë™ ë²„ì „ - ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì™„ë²½ í˜¸í™˜
"""

import cv2
import numpy as np
import torch
import json
import time
import os
from datetime import datetime
from collections import defaultdict
import asyncio
import aiohttp
from pathlib import Path
import logging
import requests

# ê¸°ì¡´ SimpleTrackerì™€ MobileNetDetector í´ë˜ìŠ¤ëŠ” ë™ì¼ (ìƒëµ)
from raspberry_integrated import SimpleTracker  # SimpleTrackerë§Œ import

# MobileNetDetectorëŠ” ìˆ˜ì •ëœ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì¬ì •ì˜
class MobileNetDetector:
    """MobileNet ê¸°ë°˜ í•´ì¶© íƒì§€ê¸°"""
    def __init__(self, model_path, confidence_threshold=0.6):
        self.device = torch.device('cpu')  # ë¼ì¦ˆë² ë¦¬íŒŒì´ëŠ” CPU ì‚¬ìš©
        self.confidence_threshold = confidence_threshold
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # ì „ì²˜ë¦¬ ì„¤ì •
        self.transform = self._get_transform()
        
        # í´ë˜ìŠ¤ ì´ë¦„ (ìˆ˜ì •ëœ 10ì¢…)
        self.class_names = [
            "ê½ƒë…¸ë‘ì´ì±„ë²Œë ˆ",     # 0 - í•´ì¶©
            "ë‹´ë°°ê°€ë£¨ì´",         # 1 - í•´ì¶©
            "ë³µìˆ­ì•„í˜¹ì§„ë”§ë¬¼",      # 2 - í•´ì¶©
            "ì©ë©ë‚˜ë¬´ë…¸ë¦°ì¬",      # 3 - í•´ì¶©
            "ë¹„ë‹¨ë…¸ë¦°ì¬",         # 4 - ì¼ë°˜ê³¤ì¶©
            "ë¨¹ë…¸ë¦°ì¬",           # 5 - ì¼ë°˜ê³¤ì¶©
            "ë¬´ìë²Œ",            # 6 - ì¼ë°˜ê³¤ì¶©
            "ë°°ì¶”ì¢€ë‚˜ë°©",         # 7 - ì¼ë°˜ê³¤ì¶©
            "ë²¼ë£©ìë²Œë ˆ",         # 8 - ì¼ë°˜ê³¤ì¶©
            "í°28ì ë°•ì´ë¬´ë‹¹ë²Œë ˆ"   # 9 - ì¼ë°˜ê³¤ì¶©
        ]
    
    def _load_model(self, model_path):
        """ëª¨ë¸ ë¡œë“œ (ONNX ë˜ëŠ” PyTorch)"""
        if model_path.endswith('.onnx'):
            import onnxruntime as ort
            return ort.InferenceSession(model_path)
        else:
            # PyTorch ëª¨ë¸
            from torchvision import models
            import torch.nn as nn
            
            model = models.mobilenet_v2(pretrained=False)
            in_features = model.classifier[1].in_features
            model.classifier = nn.Sequential(
                nn.Dropout(0.2),
                nn.Linear(in_features, 256),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(256, 10)
            )
            
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            
            return model
    
    def _get_transform(self):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜"""
        from torchvision import transforms
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    
    def detect(self, frame, roi=None):
        """í”„ë ˆì„ì—ì„œ í•´ì¶© íƒì§€"""
        detections = []
        h, w = frame.shape[:2]
        
        # ROI ì„¤ì • (í¬ì§‘ê¸° ì˜ì—­)
        if roi:
            x1, y1, x2, y2 = roi
            roi_frame = frame[y1:y2, x1:x2]
        else:
            roi_frame = frame
            x1, y1 = 0, 0
        
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ íƒì§€ (ê°„ë‹¨í•œ êµ¬í˜„)
        window_size = 128
        stride = 64
        
        for y in range(0, roi_frame.shape[0] - window_size, stride):
            for x in range(0, roi_frame.shape[1] - window_size, stride):
                window = roi_frame[y:y+window_size, x:x+window_size]
                
                # ì „ì²˜ë¦¬ ë° ì¶”ë¡ 
                input_tensor = self.transform(window).unsqueeze(0)
                
                with torch.no_grad():
                    if isinstance(self.model, torch.nn.Module):
                        outputs = self.model(input_tensor)
                        probs = torch.softmax(outputs, dim=1)
                        conf, pred_class = probs.max(1)
                    else:
                        # ONNX ì¶”ë¡ 
                        outputs = self.model.run(None, {self.model.get_inputs()[0].name: input_tensor.numpy()})
                        probs = self._softmax(outputs[0])
                        conf = probs.max()
                        pred_class = probs.argmax()
                
                # ì„ê³„ê°’ ì´ìƒë§Œ ì €ì¥
                if conf > self.confidence_threshold:
                    detections.append({
                        'bbox': [x1+x, y1+y, x1+x+window_size, y1+y+window_size],
                        'confidence': float(conf),
                        'class_id': int(pred_class),
                        'class_name': self.class_names[pred_class]
                    })
        
        return detections
    
    def _softmax(self, x):
        e_x = np.exp(x - np.max(x))
        return e_x / e_x.sum()

class SpringBootIntegratedMonitor:
    """Spring Boot ì—°ë™ í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, camera_id=0, spring_url="http://192.168.219.49:8095", ml_url="http://192.168.219.49:8003"):
        self.camera_id = camera_id
        self.spring_url = spring_url
        self.ml_url = ml_url
        self.greenhouse_id = 75  # GH_IDX
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.detector = MobileNetDetector('best_mobilenet_insect.pt')
        self.tracker = SimpleTracker()
        
        # ì¹´ë©”ë¼ ì„¤ì •
        self.cap = cv2.VideoCapture(camera_id)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.cap.set(cv2.CAP_PROP_FPS, 10)
        
        # ìƒíƒœ ê´€ë¦¬
        self.insect_counts = defaultdict(int)
        self.last_count = 0
        self.recording = False
        self.record_frames = []
        self.metadata = []
        
        # ì•Œë¦¼ ì¿¨ë‹¤ìš´ (ë©”ëª¨ë¦¬ ìºì‹œ)
        self.alert_cooldown = {}
        self.cooldown_minutes = 30
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        frame_count = 0
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                continue
            
            frame_count += 1
            
            # 3í”„ë ˆì„ë§ˆë‹¤ íƒì§€ (ì„±ëŠ¥ ìµœì í™”)
            if frame_count % 3 == 0:
                # íƒì§€ ì‹¤í–‰
                detections = self.detector.detect(frame)
                
                # ì¶”ì  ì—…ë°ì´íŠ¸
                tracked = self.tracker.update(detections)
                
                # ë§ˆë¦¿ìˆ˜ ë³€í™” ê°ì§€
                current_count = len(self.tracker.tracks)
                if current_count > self.last_count:
                    self.logger.info(f"ğŸ› ìƒˆë¡œìš´ í•´ì¶© íƒì§€! (ì´ {current_count}ë§ˆë¦¬)")
                    
                    if not self.recording:
                        # 10ì´ˆ ë…¹í™” ì‹œì‘
                        asyncio.create_task(self.start_recording(frame, tracked))
                
                self.last_count = current_count
                
                # ì‹œê°í™” (ë””ë²„ê¹…ìš©)
                self.visualize(frame, tracked)
            
            # ESC ì¢…ë£Œ
            if cv2.waitKey(1) & 0xFF == 27:
                break
        
        self.cleanup()
    
    async def start_recording(self, trigger_frame, initial_detections):
        """10ì´ˆ ë…¹í™” ë° Spring Boot ì—°ë™"""
        self.recording = True
        self.record_frames = []
        self.metadata = []
        
        timestamp = datetime.now()
        recording_start_time = time.time()
        
        # 10ì´ˆê°„ ë…¹í™”
        while time.time() - recording_start_time < 10:
            ret, frame = self.cap.read()
            if not ret:
                continue
            
            # íƒì§€ ë° ì¶”ì 
            frame_detections = self.detector.detect(frame)
            tracked = self.tracker.update(frame_detections)
            
            # í”„ë ˆì„ ì €ì¥
            self.record_frames.append(frame)
            
            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            for detection in tracked:
                bbox = detection['bbox']
                
                # í¬ë¡­ ì´ë¯¸ì§€ ìƒì„±
                x1, y1, x2, y2 = map(int, bbox)
                crop = frame[y1:y2, x1:x2]
                
                self.metadata.append({
                    'frame_id': len(self.record_frames),
                    'timestamp': (timestamp + timedelta(seconds=len(self.record_frames)/10)).isoformat(),
                    'track_id': detection['track_id'],
                    'class_id': detection['class_id'],
                    'class_name': detection['class_name'],
                    'confidence': detection['confidence'],
                    'bbox': bbox,
                    'crop_image': crop  # ì´ë¯¸ì§€ ë°ì´í„° ì§ì ‘ ì €ì¥
                })
            
            await asyncio.sleep(0.1)  # 10 FPS
        
        # 1ë‹¨ê³„: Spring Bootë¡œ ë¹„ë””ì˜¤ ì—…ë¡œë“œ
        video_path = await self.upload_to_spring_boot()
        
        # 2ë‹¨ê³„: ML ì„œë²„ë¡œ Open Set ì²˜ë¦¬ ìš”ì²­
        if video_path:
            await self.process_with_ml_server(video_path)
        
        self.recording = False
    
    async def upload_to_spring_boot(self):
        """Spring Boot VideoControllerë¡œ ë¹„ë””ì˜¤ ì—…ë¡œë“œ"""
        try:
            # ì„ì‹œ ë¹„ë””ì˜¤ íŒŒì¼ ìƒì„±
            temp_video = f"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
            h, w = self.record_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_video, fourcc, 10.0, (w, h))
            
            for frame in self.record_frames:
                out.write(frame)
            out.release()
            
            # Spring Boot API í˜¸ì¶œ
            with open(temp_video, 'rb') as video_file:
                files = {
                    'video': ('detection.mp4', video_file, 'video/mp4')
                }
                data = {
                    'camera_id': f'cam_{self.camera_id:03d}',
                    'gh_idx': self.greenhouse_id,
                    'detection_count': len(set(d['track_id'] for d in self.metadata)),
                    'recording_start_time': time.time(),
                    'frame_count': len(self.record_frames)
                }
                
                response = requests.post(
                    f"{self.spring_url}/api/video/upload",
                    files=files,
                    data=data,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    video_path = result.get('video_path')
                    img_idx = result.get('img_idx')
                    
                    self.logger.info(f"âœ… Spring Boot ì—…ë¡œë“œ ì„±ê³µ: {video_path} (IMG_IDX: {img_idx})")
                    
                    # ë©”íƒ€ë°ì´í„°ì— img_idx ì¶”ê°€
                    for item in self.metadata:
                        item['img_idx'] = img_idx
                    
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    os.remove(temp_video)
                    
                    return video_path
                else:
                    self.logger.error(f"âŒ Spring Boot ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                    return None
                    
        except Exception as e:
            self.logger.error(f"âŒ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    async def process_with_ml_server(self, video_path):
        """ML ì„œë²„ë¡œ Open Set ì²˜ë¦¬ ë° ì•Œë¦¼ ìš”ì²­"""
        try:
            # í¬ë¡­ ì´ë¯¸ì§€ì™€ ë©”íƒ€ë°ì´í„° ì „ì†¡
            async with aiohttp.ClientSession() as session:
                # ë©”íƒ€ë°ì´í„° ì „ì†¡
                metadata_payload = {
                    'greenhouse_id': self.greenhouse_id,
                    'video_path': video_path,
                    'detections': []
                }
                
                # ê° íƒì§€ì— ëŒ€í•´ Open Set ì²˜ë¦¬
                for idx, detection in enumerate(self.metadata):
                    # í¬ë¡­ ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
                    crop_image = detection.pop('crop_image')
                    _, buffer = cv2.imencode('.jpg', crop_image)
                    image_base64 = buffer.tobytes().hex()
                    
                    detection['crop_base64'] = image_base64
                    metadata_payload['detections'].append(detection)
                
                # Open Set Recognition ìš”ì²­
                async with session.post(
                    f"{self.ml_url}/api/process-detections",
                    json=metadata_payload
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        
                        # ì•Œë¦¼ ì²˜ë¦¬
                        await self.handle_alerts(result)
                        
                        self.logger.info(f"âœ… ML ì²˜ë¦¬ ì™„ë£Œ: {result.get('processed_count')}ê°œ íƒì§€")
                    else:
                        self.logger.error(f"âŒ ML ì²˜ë¦¬ ì‹¤íŒ¨: {resp.status}")
                        
        except Exception as e:
            self.logger.error(f"âŒ ML ì„œë²„ ì—°ë™ ì˜¤ë¥˜: {e}")
    
    async def handle_alerts(self, ml_result):
        """ì•Œë¦¼ ì²˜ë¦¬ (SignalWire ì „í™” ë“±)"""
        alerts = ml_result.get('alerts', [])
        
        for alert in alerts:
            insect_type = alert['insect_type']
            confidence = alert['confidence']
            
            # ì¿¨ë‹¤ìš´ ì²´í¬
            if self.check_cooldown(insect_type):
                # SignalWire ì „í™” ë°œì‹  ìš”ì²­
                await self.make_phone_call(insect_type, confidence)
                
                # DBì— ì•Œë¦¼ ê¸°ë¡ (Spring Boot API í†µí•´)
                await self.save_alert_to_db(alert)
    
    def check_cooldown(self, insect_type):
        """ì•Œë¦¼ ì¿¨ë‹¤ìš´ ì²´í¬"""
        key = f"{self.greenhouse_id}_{insect_type}"
        now = datetime.now()
        
        if key in self.alert_cooldown:
            last_alert = self.alert_cooldown[key]
            if (now - last_alert).seconds < self.cooldown_minutes * 60:
                remaining = self.cooldown_minutes - (now - last_alert).seconds // 60
                self.logger.info(f"â° ì•Œë¦¼ ì¿¨ë‹¤ìš´: {insect_type} ({remaining}ë¶„ ë‚¨ìŒ)")
                return False
        
        self.alert_cooldown[key] = now
        return True
    
    async def make_phone_call(self, insect_type, confidence):
        """SignalWire ì „í™” ë°œì‹ """
        try:
            async with aiohttp.ClientSession() as session:
                # ì „í™”ë²ˆí˜¸ ì¡°íšŒ
                async with session.get(
                    f"{self.spring_url}/ml/user-phone-by-ghidx",
                    params={'gh_idx': self.greenhouse_id}
                ) as resp:
                    if resp.status == 200:
                        phone_data = await resp.json()
                        phone_number = phone_data.get('userPhone')
                        
                        # SignalWire ë°œì‹ 
                        call_data = {
                            'to_number': phone_number,
                            'message': f"ë†ì¥ì—ì„œ {insect_type}ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹ ë¢°ë„ëŠ” {confidence:.0%}ì…ë‹ˆë‹¤."
                        }
                        
                        async with session.post(
                            f"{self.ml_url}/api/make-call",
                            json=call_data
                        ) as call_resp:
                            if call_resp.status == 200:
                                self.logger.info(f"ğŸ“ ì „í™” ë°œì‹  ì„±ê³µ: {phone_number}")
                            else:
                                self.logger.error(f"âŒ ì „í™” ë°œì‹  ì‹¤íŒ¨")
                                
        except Exception as e:
            self.logger.error(f"âŒ ì „í™” ë°œì‹  ì˜¤ë¥˜: {e}")
    
    async def save_alert_to_db(self, alert):
        """ì•Œë¦¼ DB ì €ì¥ (Spring Boot ê²½ìœ )"""
        try:
            # Spring Bootì˜ AlertController í†µí•´ ì €ì¥
            alert_data = {
                'gh_idx': self.greenhouse_id,
                'insect_idx': alert['class_id'],
                'insect_name': alert['insect_type'],
                'confidence': alert['confidence'],
                'img_idx': alert.get('img_idx'),
                'created_at': datetime.now().isoformat()
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.spring_url}/api/alert/save",
                    json=alert_data
                ) as resp:
                    if resp.status == 200:
                        self.logger.info("ğŸ’¾ ì•Œë¦¼ DB ì €ì¥ ì™„ë£Œ")
                    else:
                        self.logger.error("âŒ ì•Œë¦¼ DB ì €ì¥ ì‹¤íŒ¨")
                        
        except Exception as e:
            self.logger.error(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def visualize(self, frame, detections):
        """ë””ë²„ê¹…ìš© ì‹œê°í™”"""
        for detection in detections:
            bbox = detection['bbox']
            x1, y1, x2, y2 = map(int, bbox)
            
            # ë°”ìš´ë”© ë°•ìŠ¤
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ë¼ë²¨
            label = f"{detection['class_name']} #{detection['track_id']}"
            conf = f"{detection['confidence']:.2f}"
            cv2.putText(frame, f"{label} {conf}", (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # ìƒíƒœ í‘œì‹œ
        status = "ğŸ”´ Recording" if self.recording else "âšª Monitoring"
        info = f"{status} | Total: {len(self.tracker.tracks)}"
        cv2.putText(frame, info, (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        cv2.imshow('Spring Boot Integrated Monitor', frame)
    
    def cleanup(self):
        """ì¢…ë£Œ ì²˜ë¦¬"""
        self.cap.release()
        cv2.destroyAllWindows()
        self.logger.info("ğŸ”š ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")

if __name__ == "__main__":
    # ì‹¤í–‰
    monitor = SpringBootIntegratedMonitor(
        camera_id=0,
        spring_url="http://192.168.219.49:8095",
        ml_url="http://192.168.219.49:8003"
    )
    asyncio.run(monitor.run())

# ì„¤ì¹˜ í•„ìš”:
# pip install opencv-python torch torchvision aiohttp requests