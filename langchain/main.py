from dotenv import load_dotenv
import os

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ChatMessageHistory
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# âœ… 1. API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… 2. ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
embedding = OpenAIEmbeddings(model="text-embedding-3-large", openai_api_key=OPENAI_API_KEY)
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embedding)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# âœ… 3. LLM ì •ì˜
chat = ChatOpenAI(model="gpt-4o-mini", openai_api_key=OPENAI_API_KEY)

# âœ… 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ contextì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë¼. "
            "ë§Œì•½ contextì— ë‹µì´ ì—†ìœ¼ë©´ 'ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.' ë¼ê³  ë‹µí•˜ë¼.\n\n{context}",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

# âœ… 5. ë¬¸ì„œ â†’ ë‹µë³€ ìƒì„± ì²´ì¸ êµ¬ì„±
document_chain = create_stuff_documents_chain(chat, question_answering_prompt)

# âœ… 6. ì „ì²´ RAG ì²´ì¸ ìƒì„±
rag_chain = create_retrieval_chain(retriever, document_chain)

# âœ… 7. ì±„íŒ… íˆìŠ¤í† ë¦¬ ê°ì²´ ìƒì„±
chat_history = ChatMessageHistory()

# âœ… 8. ìœ ì € ì§ˆë¬¸
question = "í† ë§ˆí†  ë¿”ë‚˜ë°©ì€ ì–´ë–¤ í”¼í•´ë¥¼ ì…íˆê³ , ì–´ë–»ê²Œ ë°©ì œí•˜ë‚˜ìš”?"
chat_history.add_user_message(question)

# âœ… 9. RAG ì‹¤í–‰
response = rag_chain.invoke({
    "messages": chat_history.messages
})

# âœ… 10. ì‘ë‹µ ì €ì¥ ë° ì¶œë ¥
chat_history.add_ai_message(response["answer"])
print("ğŸ§  GPT ì‘ë‹µ:")
print(response["answer"])
