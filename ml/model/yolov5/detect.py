import argparse
import os
import time
from pathlib import Path
from datetime import datetime
import torch
import requests
import cv2
from models.common import DetectMultiBackend
from utils.dataloaders import LoadStreams, LoadImages
from utils.general import (
    check_img_size, check_imshow, check_requirements,
    increment_path, non_max_suppression,
    scale_boxes
)
from utils.torch_utils import select_device, smart_inference_mode
from dotenv import load_dotenv
import subprocess
from ultralytics.utils.plotting import Annotator, colors
load_dotenv()

# ê³ ì • GH_IDX
gh_idx = 74


# ì „í™”ë²ˆí˜¸ì™€ ì „í™” ê¸°ëŠ¥ì€ ML API ì„œë²„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ì´ê´€

# ì „í™” ë°œì‹ ì€ ML API ì„œë²„ì—ì„œ Spring Bootë¥¼ í†µí•´ ì²˜ë¦¬


def get_insect_idx(name):
    return {
        "ê½ƒë…¸ë‘ì´ì±„ë²Œë ˆ": 1,
        "ë‹´ë°°ê°€ë£¨ì´": 2,
        "ë¹„ë‹¨ë…¸ë¦°ì¬": 3,
        "ì•Œë½ìˆ˜ì—¼ë…¸ë¦°ì¬": 4
    }.get(name, 0)

# ğŸ› íƒì§€ ê²°ê³¼ API ì „ì†¡
def send_detection_to_api(insect_name, confidence, img_idx):
    now = datetime.now()
    created_at = now.strftime("%Y-%m-%d %H:%M:%S")

    payload = {
        "anlsModel": "YOLOv5",
        "anlsContent": f"{insect_name} {confidence * 100:.2f}%ë¡œ íƒì§€ì™„ë£Œ",
        "anlsResult": insect_name,
        "createdAt": created_at,
        "insectIdx": get_insect_idx(insect_name),
        "imgIdx": img_idx,
        "notiCheck": 'N',
        "ghIdx": gh_idx,
        "anlsAcc": int(confidence * 100)
    }

    try:
        res = requests.post("http://localhost:8095/api/qc-classification", json=payload)
        print(f"[ì „ì†¡] {insect_name} ì €ì¥ ì™„ë£Œ | ì‹ ë¢°ë„: {confidence:.2f} | ìƒíƒœì½”ë“œ: {res.status_code}")
    except Exception as e:
        print("[ì „ì†¡ ì‹¤íŒ¨]", e)

# ğŸ¥ ì˜ìƒ ì—…ë¡œë“œ í•¨ìˆ˜
def upload_video(file_path, class_id, gh_idx):
    url = "http://localhost:8095/api/qc-videos"
    files = {"video": open(file_path, "rb")}
    data = {"classId": class_id, "ghIdx": gh_idx}
    try:
        res = requests.post(url, files=files, data=data)
        print(f"[ì„œë²„ ì‘ë‹µ ìƒíƒœì½”ë“œ] {res.status_code}")
        print(f"[ì„œë²„ ì‘ë‹µ ë³¸ë¬¸] {res.text}")
        if res.status_code == 200:
            json_res = res.json()
            return json_res.get("imgIdx")
    except Exception as e:
        print("[ì˜ìƒ ì „ì†¡ ì—ëŸ¬]", e)
    return None

@smart_inference_mode()
def run(weights=Path("best_clean.pt"), source=0, data=Path("data/coco128.yaml"), imgsz=(640, 640),
        conf_thres=0.25, iou_thres=0.45, device="", view_img=True):

    device = select_device(device)
    model = DetectMultiBackend(weights, device=device, data=data, fp16=False)
    stride, names, pt = model.stride, model.names, model.pt
    imgsz = check_img_size(imgsz, s=stride)

    dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)
    model.warmup(imgsz=(1, 3, *imgsz))

    save_dir = Path("clips")
    save_dir.mkdir(parents=True, exist_ok=True)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    
    # ì‹¤ì œ ì›¹ìº  FPS ê°€ì ¸ì˜¤ê¸°
    cap = cv2.VideoCapture(source)
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    cap.release()
    
    # YOLOv5 ì¶”ë¡  ì†ë„ì— ë§ê²Œ ë‚®ì€ FPS ì„¤ì •
    fps = 7  # ì‹¤ì œ ì²˜ë¦¬ ê°€ëŠ¥í•œ FPSë¡œ ê³ ì •
    
    print(f"[ì„¤ì •] ë…¹í™” FPS: {fps}, ì§€ì†ì‹œê°„: 10ì´ˆ")

    frame_buffer = []
    recording = False
    start_time = None
    duration = 10
    insect_name = ""
    best_conf = 0
    video_path = ""
    frame_count = 0  # ì‹¤ì œ ë…¹í™”ëœ í”„ë ˆì„ ìˆ˜ ì¹´ìš´íŠ¸

    # ë²Œë ˆ íƒì§€ ì¿¨ë‹¤ìš´
    last_detection_time = 0
    DETECTION_COOLDOWN = 30 # ì´ˆ ë‹¨ìœ„ 

    for path, im, im0s, vid_cap, s in dataset:
        im = torch.from_numpy(im).to(model.device).float() / 255.0
        if im.ndim == 3:
            im = im[None]

        pred = model(im)
        pred = non_max_suppression(pred, conf_thres, iou_thres, max_det=1000)

        im0 = im0s[0].copy()
        annotator = Annotator(im0, line_width=3, example=str(names))

        detected = False

        for det in pred:
            if len(det):
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()
                for *xyxy, conf, cls in reversed(det):
                    #print(f"[íƒì§€ ë¡œê·¸] í´ë˜ìŠ¤: {names[int(cls)]}, ì‹ ë¢°ë„: {conf:.2f}, ì¢Œí‘œ: {xyxy}")
                    cls_id = int(cls)
                    insect_name = names[cls_id]
                    confidence = float(conf)
                    label = f"{insect_name} {confidence:.2f}"
                    annotator.box_label(xyxy, label, color=colors(cls_id, True))
                    detected = True

                    now = time.time()
                    if not recording and (now - last_detection_time) > DETECTION_COOLDOWN:
                        last_detection_time = now
                        start_time = time.time()
                        video_name = f"{insect_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
                        video_path = str(save_dir / video_name)
                        out = cv2.VideoWriter(video_path, fourcc, fps, (im0.shape[1], im0.shape[0]))
                        print(f"[ë…¹í™” ì‹œì‘] {video_path} | íƒì§€ëœ ë²Œë ˆ: {insect_name} | ì‹ ë¢°ë„: {confidence:.2f}")
                        print(f"[ì„¤ì •] FPS: {fps}, ì˜ˆìƒ í”„ë ˆì„ ìˆ˜: {fps * duration}")
                        best_conf = confidence
                        recording = True
                        frame_count = 0

        annotated_frame = annotator.result()

        if recording:
            out.write(annotated_frame)
            frame_count += 1
            if time.time() - start_time > duration:
                recording = False
                out.release() 
                actual_duration = time.time() - start_time
                print(f"[ë…¹í™” ì¢…ë£Œ] ì‹¤ì œ ì‹œê°„: {actual_duration:.1f}ì´ˆ, í”„ë ˆì„ ìˆ˜: {frame_count}, ì‹¤ì œ FPS: {frame_count/actual_duration:.1f}")

                converted_path = video_path.replace(".mp4", "_h264.mp4")
                
                # VideoWriter FPS ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ë³€í™˜
                print(f"[ë³€í™˜] ì›ë³¸ FPS ìœ ì§€: {fps}")
                
                # ğŸ”‡ ffmpeg ë¡œê·¸ ìˆ¨ê¸°ê¸°
                with open(os.devnull, 'w') as devnull:
                    subprocess.run(
                [
                    'ffmpeg', '-y',
                    '-i', video_path,
                    '-c:v', 'libx264',
                    '-c:a', 'aac', 
                    '-preset', 'fast',
                    converted_path
                ],
                stdout=devnull,
                stderr=devnull
            )
                os.remove(video_path)
                video_path = converted_path

                class_id = get_insect_idx(insect_name)
                img_idx = upload_video(video_path, class_id, gh_idx)
                if img_idx:
                    time.sleep(1)
                    send_detection_to_api(insect_name, best_conf, img_idx)
                    # make_call_by_gh_idx(gh_idx)
                    # ì£¼ì„ í’€ë©´ ì „í™” ê°€ëŠ¥

                    # GPT ìš”ì•½ì€ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í•„ìš”ì‹œ ìš”ì²­í•˜ë„ë¡ ë³€ê²½
                    print(f"[ì™„ë£Œ] í•´ì¶© íƒì§€ ë° ì˜ìƒ ì—…ë¡œë“œ ì™„ë£Œ | IMG_IDX: {img_idx}")

        if view_img:
            cv2.imshow("YOLOv5", annotated_frame)
            if cv2.waitKey(1) == ord("q"):
                break

def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument("--weights", type=str, default="best_clean.pt")
    parser.add_argument("--source", type=str, default="0")
    parser.add_argument("--imgsz", nargs="+", type=int, default=[640])
    parser.add_argument("--conf-thres", type=float, default=0.25)
    parser.add_argument("--iou-thres", type=float, default=0.45)
    parser.add_argument("--device", default="")
    parser.add_argument("--view-img", action="store_true")
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1
    return opt

def main(opt):
    check_requirements(exclude=("tensorboard", "thop"))
    run(**vars(opt))

if __name__ == "__main__":
    opt = parse_opt()
    main(opt)
