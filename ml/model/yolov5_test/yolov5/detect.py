import argparse
import os
import time
from pathlib import Path
from datetime import datetime
import torch
import requests
import cv2
from twilio.rest import Client
from ultralytics.utils.plotting import Annotator, colors
from models.common import DetectMultiBackend
from utils.dataloaders import LoadStreams, LoadImages
from utils.general import (
    check_img_size, check_imshow, check_requirements,
    increment_path, non_max_suppression,
    scale_boxes
)
from utils.torch_utils import select_device, smart_inference_mode
from urllib.parse import quote
from signalwire.rest import Client as SignalWireClient
from dotenv import load_dotenv
import subprocess
load_dotenv()
# ê³ ì • GH_IDX
gh_idx = 1

# Twilio ì„¤ì •
TWILIO_ACCOUNT_SID = os.getenv("TWILIO_ACCOUNT_SID")
TWILIO_AUTH_TOKEN = os.getenv("TWILIO_AUTH_TOKEN")
TWILIO_PHONE_NUMBER = os.getenv("TWILIO_PHONE_NUMBER")
USER_PHONE_NUMBER = os.getenv("USER_PHONE_NUMBER")  # ìˆ˜ì‹ ì
PUBLIC_FASTAPI_BASE = "https://a42af3bf7b23.ngrok-free.app"

# ì „í™” ì¿¨ë‹¤ìš´
last_call_time = 0
CALL_COOLDOWN = 60 #ì´ˆë‹¨ìœ„ ì‹¤ì‚¬ìš©ì‹œ 10ë¶„ìœ¼ë¡œ ë³€ê²½

# SIGNALWIRE_PROJECT_ID = os.getenv("SIGNALWIRE_PROJECT_ID")
# SIGNALWIRE_AUTH_TOKEN = os.getenv("SIGNALWIRE_AUTH_TOKEN")
# SIGNALWIRE_PHONE_NUMBER = os.getenv("SIGNALWIRE_PHONE_NUMBER")
# SIGNALWIRE_SPACE_URL = os.getenv("SIGNALWIRE_SPACE_URL")

# # í…ŒìŠ¤íŠ¸ìš© ìˆ˜ì‹ ì ë²ˆí˜¸
# TEST_PHONE_NUMBER = "+821085849748"  # â† í…ŒìŠ¤íŠ¸í•  ì‹¤ì œ ì „í™”ë²ˆí˜¸ë¡œ ë°”ê¿”ì£¼ì„¸ìš”

# def make_call(insect_name: str, confidence: float):
#     client = SignalWireClient(
#         SIGNALWIRE_PROJECT_ID,
#         SIGNALWIRE_AUTH_TOKEN,
#         signalwire_space_url=SIGNALWIRE_SPACE_URL
#     )

#     url = f"{PUBLIC_FASTAPI_BASE}/twilio-call?insect={quote(insect_name)}"

#     try:
#         call = client.calls.create(
#             from_=SIGNALWIRE_PHONE_NUMBER,
#             to=TEST_PHONE_NUMBER,
#             url=url
#         )
#         print(f"[í…ŒìŠ¤íŠ¸ ì „í™” ë°œì‹ ] Call SID: {call.sid} | ëŒ€ìƒ: {TEST_PHONE_NUMBER}")
#     except Exception as e:
#         print("[ì „í™” ë°œì‹  ì‹¤íŒ¨]", e)


def make_call(insect_name: str, confidence: float):
    global last_call_time
    now = time.time()
    if now - last_call_time < CALL_COOLDOWN:
        print(f"[ì „í™” ê±´ë„ˆëœ€] ìµœê·¼ì— ë°œì‹ ë¨ ({now-last_call_time:.1f}s ì „)")
        return
    try:
        client = Client(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN)
        url = f"{PUBLIC_FASTAPI_BASE}/twilio-call"
        call = client.calls.create(
            to=USER_PHONE_NUMBER,
            from_=TWILIO_PHONE_NUMBER,
            url=url
        )
        last_call_time = now
        print(f"[ì „í™” ë°œì‹ ] Call SID: {call.sid}")
    except Exception as e:
        print("[ì „í™” ì˜¤ë¥˜]", e)

def get_insect_idx(name):
    return {
        "ê½ƒë…¸ë‘ì´ì±„ë²Œë ˆ": 1,
        "ë‹´ë°°ê°€ë£¨ì´": 2,
        "ë¹„ë‹¨ë…¸ë¦°ì¬": 3,
        "ì•Œë½ìˆ˜ì—¼ë…¸ë¦°ì¬": 4
    }.get(name, 0)

# ğŸ› íƒì§€ ê²°ê³¼ API ì „ì†¡
def send_detection_to_api(insect_name, confidence, img_idx):
    now = datetime.now()
    created_at = now.strftime("%Y-%m-%d %H:%M:%S")

    payload = {
        "anlsModel": "YOLOv5",
        "anlsContent": f"{insect_name} {confidence * 100:.2f}%ë¡œ íƒì§€ì™„ë£Œ",
        "anlsResult": insect_name,
        "createdAt": created_at,
        "insectIdx": get_insect_idx(insect_name),
        "imgIdx": img_idx,
        "notiCheck": 'N',
        "ghIdx": gh_idx,
        "anlsAcc": int(confidence * 100)
    }

    try:
        res = requests.post("http://localhost:8095/api/qc-classification", json=payload)
        print(f"[ì „ì†¡] {insect_name} ì €ì¥ ì™„ë£Œ | ì‹ ë¢°ë„: {confidence:.2f} | ìƒíƒœì½”ë“œ: {res.status_code}")
    except Exception as e:
        print("[ì „ì†¡ ì‹¤íŒ¨]", e)

# ğŸ¥ ì˜ìƒ ì—…ë¡œë“œ í•¨ìˆ˜
def upload_video(file_path, class_id, gh_idx):
    url = "http://localhost:8095/api/qc-videos"
    files = {"video": open(file_path, "rb")}
    data = {"classId": class_id, "ghIdx": gh_idx}
    try:
        res = requests.post(url, files=files, data=data)
        print(f"[ì„œë²„ ì‘ë‹µ ìƒíƒœì½”ë“œ] {res.status_code}")
        print(f"[ì„œë²„ ì‘ë‹µ ë³¸ë¬¸] {res.text}")
        if res.status_code == 200:
            json_res = res.json()
            return json_res.get("imgIdx")
    except Exception as e:
        print("[ì˜ìƒ ì „ì†¡ ì—ëŸ¬]", e)
    return None

@smart_inference_mode()
def run(weights=Path("best_clean.pt"), source=0, data=Path("data/coco128.yaml"), imgsz=(640, 640),
        conf_thres=0.25, iou_thres=0.45, device="", view_img=True):

    device = select_device(device)
    model = DetectMultiBackend(weights, device=device, data=data, fp16=False)
    stride, names, pt = model.stride, model.names, model.pt
    imgsz = check_img_size(imgsz, s=stride)

    dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)
    model.warmup(imgsz=(1, 3, *imgsz))

    save_dir = Path("clips")
    save_dir.mkdir(parents=True, exist_ok=True)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')

    frame_buffer = []
    recording = False
    start_time = None
    fps = 30
    duration = 10
    insect_name = ""
    best_conf = 0
    video_path = ""

    # ë²Œë ˆ íƒì§€ ì¿¨ë‹¤ìš´
    last_detection_time = 0
    DETECTION_COOLDOWN = 30 # ì´ˆ ë‹¨ìœ„ 

    for path, im, im0s, vid_cap, s in dataset:
        im = torch.from_numpy(im).to(model.device).float() / 255.0
        if im.ndim == 3:
            im = im[None]

        pred = model(im)
        pred = non_max_suppression(pred, conf_thres, iou_thres, max_det=1000)

        im0 = im0s[0].copy()
        annotator = Annotator(im0, line_width=3, example=str(names))

        detected = False

        for det in pred:
            if len(det):
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()
                for *xyxy, conf, cls in reversed(det):
                    print(f"[íƒì§€ ë¡œê·¸] í´ë˜ìŠ¤: {names[int(cls)]}, ì‹ ë¢°ë„: {conf:.2f}, ì¢Œí‘œ: {xyxy}")
                    cls_id = int(cls)
                    insect_name = names[cls_id]
                    confidence = float(conf)
                    label = f"{insect_name} {confidence:.2f}"
                    annotator.box_label(xyxy, label, color=colors(cls_id, True))
                    detected = True

                    now = time.time()
                    if not recording and (now - last_detection_time) > DETECTION_COOLDOWN:
                        last_detection_time = now
                        start_time = time.time()
                        video_name = f"{insect_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
                        video_path = str(save_dir / video_name)
                        out = cv2.VideoWriter(video_path, fourcc, fps, (im0.shape[1], im0.shape[0]))
                        print(f"[ë…¹í™” ì‹œì‘] {video_path} | íƒì§€ëœ ë²Œë ˆ: {insect_name} | ì‹ ë¢°ë„: {confidence:.2f}")
                        best_conf = confidence
                        recording = True

        annotated_frame = annotator.result()

        if recording:
            out.write(annotated_frame)
            if time.time() - start_time > duration:
                recording = False
                out.release()
                print("[ë…¹í™” ì¢…ë£Œ]")

                converted_path = video_path.replace(".mp4", "_h264.mp4")
                 # ğŸ”‡ ffmpeg ë¡œê·¸ ìˆ¨ê¸°ê¸°
                with open(os.devnull, 'w') as devnull:
                    subprocess.run(
                [
                    'ffmpeg', '-y',
                    '-i', video_path,
                    '-vcodec', 'libx264',
                    '-acodec', 'aac',
                    converted_path
                ],
                stdout=devnull,
                stderr=devnull
            )
                os.remove(video_path)
                video_path = converted_path

                class_id = get_insect_idx(insect_name)
                img_idx = upload_video(video_path, class_id, gh_idx)
                if img_idx:
                    time.sleep(1)
                    send_detection_to_api(insect_name, best_conf, img_idx)
                    make_call(insect_name, best_conf)

                    try:
                        gpt_res = requests.get(f"http://localhost:8000/api/summary-by-imgidx?imgIdx={img_idx}")
                        if gpt_res.status_code == 200:
                            print("[GPT] ìš”ì•½ ì‘ë‹µ ì €ì¥ ì™„ë£Œ")
                        else:
                            print("[GPT] ìš”ì•½ ìš”ì²­ ì‹¤íŒ¨", gpt_res.text)
                    except Exception as e:
                        print("[GPT] ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)

        if view_img:
            cv2.imshow("YOLOv5", annotated_frame)
            if cv2.waitKey(1) == ord("q"):
                break

def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument("--weights", type=str, default="best_clean.pt")
    parser.add_argument("--source", type=str, default="0")
    parser.add_argument("--imgsz", nargs="+", type=int, default=[640])
    parser.add_argument("--conf-thres", type=float, default=0.25)
    parser.add_argument("--iou-thres", type=float, default=0.45)
    parser.add_argument("--device", default="")
    parser.add_argument("--view-img", action="store_true")
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1
    return opt

def main(opt):
    check_requirements(exclude=("tensorboard", "thop"))
    run(**vars(opt))

if __name__ == "__main__":
    opt = parse_opt()
    main(opt)
