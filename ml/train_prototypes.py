#!/usr/bin/env python3
"""
Open Set Recognitionì„ ìœ„í•œ í”„ë¡œí† íƒ€ì… í•™ìŠµ
4ì¢… í•´ì¶©ì˜ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ ë° ì €ì¥
"""

import torch
import torch.nn as nn
from torchvision import models, transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import numpy as np
import os
import json
from tqdm import tqdm
import pickle
from sklearn.metrics import confusion_matrix, classification_report

# 4ì¢… í•´ì¶©ë§Œ í•™ìŠµ (0-3ë²ˆì´ í•´ì¶©)
PEST_CLASSES = {
    0: "ê½ƒë…¸ë‘ì´ì±„ë²Œë ˆ",
    1: "ë‹´ë°°ê°€ë£¨ì´",
    2: "ë³µìˆ­ì•„í˜¹ì§„ë”§ë¬¼",
    3: "ì©ë©ë‚˜ë¬´ë…¸ë¦°ì¬"
}

# ì „ì²´ 10ì¢… (í…ŒìŠ¤íŠ¸ìš©)
ALL_CLASSES = {
    0: "ê½ƒë…¸ë‘ì´ì±„ë²Œë ˆ",     # í•´ì¶© O
    1: "ë‹´ë°°ê°€ë£¨ì´",         # í•´ì¶© O  
    2: "ë³µìˆ­ì•„í˜¹ì§„ë”§ë¬¼",      # í•´ì¶© O
    3: "ì©ë©ë‚˜ë¬´ë…¸ë¦°ì¬",      # í•´ì¶© O
    4: "ë¹„ë‹¨ë…¸ë¦°ì¬",         # í•´ì¶© X (Unknown)
    5: "ë¨¹ë…¸ë¦°ì¬",           # í•´ì¶© X (Unknown)
    6: "ë¬´ìë²Œ",            # í•´ì¶© X (Unknown)
    7: "ë°°ì¶”ì¢€ë‚˜ë°©",         # í•´ì¶© X (Unknown)
    8: "ë²¼ë£©ìë²Œë ˆ",         # í•´ì¶© X (Unknown)
    9: "í°28ì ë°•ì´ë¬´ë‹¹ë²Œë ˆ"   # í•´ì¶© X (Unknown)
}

class PestDataset(Dataset):
    """4ì¢… í•´ì¶© ë°ì´í„°ì…‹ (í•™ìŠµìš©)"""
    def __init__(self, root_dir, transform=None, mode='train'):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = []
        
        # 4ì¢… í•´ì¶© ë°ì´í„°ë§Œ ë¡œë“œ
        for class_id, class_name in PEST_CLASSES.items():
            class_dir = os.path.join(root_dir, mode, class_name)
            if os.path.exists(class_dir):
                for img_name in os.listdir(class_dir):
                    if img_name.endswith(('.jpg', '.png', '.jpeg')):
                        self.samples.append({
                            'path': os.path.join(class_dir, img_name),
                            'label': class_id,
                            'name': class_name
                        })
        
        print(f"âœ… {mode}: {len(self.samples)}ê°œ í•´ì¶© ì´ë¯¸ì§€ ë¡œë“œ")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        image = Image.open(sample['path']).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, sample['label']

class FullDataset(Dataset):
    """ì „ì²´ 10ì¢… ë°ì´í„°ì…‹ (í…ŒìŠ¤íŠ¸ìš©)"""
    def __init__(self, root_dir, transform=None, mode='test'):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = []
        
        # ì „ì²´ 10ì¢… ë°ì´í„° ë¡œë“œ
        for class_id, class_name in ALL_CLASSES.items():
            class_dir = os.path.join(root_dir, mode, class_name)
            if os.path.exists(class_dir):
                for img_name in os.listdir(class_dir):
                    if img_name.endswith(('.jpg', '.png', '.jpeg')):
                        self.samples.append({
                            'path': os.path.join(class_dir, img_name),
                            'label': class_id,
                            'name': class_name,
                            'is_known': class_id < 4  # 0-3ë²ˆë§Œ Known
                        })
        
        known_count = sum(1 for s in self.samples if s['is_known'])
        unknown_count = len(self.samples) - known_count
        print(f"âœ… {mode}: ì´ {len(self.samples)}ê°œ (Known: {known_count}, Unknown: {unknown_count})")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        image = Image.open(sample['path']).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, sample['label'], sample['is_known']

def extract_features(model, dataloader, device):
    """íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
    model.eval()
    features_dict = {i: [] for i in range(4)}  # 4ì¢…
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="íŠ¹ì§• ì¶”ì¶œ ì¤‘"):
            images = images.to(device)
            
            # ResNet50 íŠ¹ì§• ì¶”ì¶œ
            features = model(images)
            features = features.squeeze().cpu().numpy()
            
            # í´ë˜ìŠ¤ë³„ë¡œ ì €ì¥
            for feat, label in zip(features, labels):
                # L2 ì •ê·œí™”
                feat_norm = feat / np.linalg.norm(feat)
                features_dict[label.item()].append(feat_norm)
    
    return features_dict

def compute_prototypes(features_dict):
    """í´ë˜ìŠ¤ë³„ í”„ë¡œí† íƒ€ì… ê³„ì‚° (í‰ê·  íŠ¹ì§• ë²¡í„°)"""
    prototypes = {}
    
    for class_id, features in features_dict.items():
        if features:
            # í‰ê·  ê³„ì‚°
            prototype = np.mean(features, axis=0)
            # ë‹¤ì‹œ ì •ê·œí™”
            prototype = prototype / np.linalg.norm(prototype)
            prototypes[class_id] = prototype
            
            print(f"í´ë˜ìŠ¤ {PEST_CLASSES[class_id]}: {len(features)}ê°œ ìƒ˜í”Œì—ì„œ í”„ë¡œí† íƒ€ì… ìƒì„±")
    
    return prototypes

def compute_thresholds(features_dict, prototypes, percentile=5):
    """í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ê³„ì‚° (ê°™ì€ í´ë˜ìŠ¤ ë‚´ ê±°ë¦¬ ë¶„í¬ ê¸°ë°˜)"""
    thresholds = {}
    
    for class_id, features in features_dict.items():
        if class_id in prototypes and features:
            prototype = prototypes[class_id]
            
            # í”„ë¡œí† íƒ€ì…ê³¼ ê° ìƒ˜í”Œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarities = []
            for feat in features:
                similarity = np.dot(feat, prototype)
                similarities.append(similarity)
            
            # ë°±ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ì„ê³„ê°’ (í•˜ìœ„ 5%)
            threshold = np.percentile(similarities, percentile)
            thresholds[class_id] = max(0.5, threshold)  # ìµœì†Œ 0.5
            
            print(f"í´ë˜ìŠ¤ {PEST_CLASSES[class_id]}: ì„ê³„ê°’ = {thresholds[class_id]:.3f} "
                  f"(min={min(similarities):.3f}, max={max(similarities):.3f})")
    
    return thresholds

def test_open_set(model, test_loader, prototypes, thresholds, device):
    """Open Set Recognition í…ŒìŠ¤íŠ¸"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    all_is_known = []
    all_max_similarities = []
    
    with torch.no_grad():
        for images, labels, is_known in tqdm(test_loader, desc="í…ŒìŠ¤íŠ¸ ì¤‘"):
            images = images.to(device)
            
            # íŠ¹ì§• ì¶”ì¶œ
            features = model(images)
            features = features.squeeze().cpu().numpy()
            
            # ê° ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡
            for feat, label, known in zip(features, labels, is_known):
                # L2 ì •ê·œí™”
                feat_norm = feat / np.linalg.norm(feat)
                
                # ëª¨ë“  í”„ë¡œí† íƒ€ì…ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = {}
                for class_id, prototype in prototypes.items():
                    similarity = np.dot(feat_norm, prototype)
                    similarities[class_id] = similarity
                
                # ê°€ì¥ ìœ ì‚¬í•œ í´ë˜ìŠ¤ ì°¾ê¸°
                best_class = max(similarities, key=similarities.get)
                max_similarity = similarities[best_class]
                
                # ì„ê³„ê°’ í™•ì¸ (Unknown íŒë‹¨)
                if max_similarity >= thresholds[best_class]:
                    prediction = best_class
                else:
                    prediction = -1  # Unknown
                
                all_predictions.append(prediction)
                all_labels.append(label.item() if label.item() < 4 else -1)  # 4-9ë²ˆì€ Unknownìœ¼ë¡œ
                all_is_known.append(known.item())
                all_max_similarities.append(max_similarity)
    
    return all_predictions, all_labels, all_is_known, all_max_similarities

def evaluate_results(predictions, labels, is_known, similarities):
    """ê²°ê³¼ í‰ê°€ ë° ì¶œë ¥"""
    predictions = np.array(predictions)
    labels = np.array(labels)
    is_known = np.array(is_known)
    similarities = np.array(similarities)
    
    print("\n" + "="*60)
    print("ğŸ“Š Open Set Recognition í‰ê°€ ê²°ê³¼")
    print("="*60)
    
    # 1. Known í´ë˜ìŠ¤ ì •í™•ë„
    known_mask = is_known == True
    if known_mask.sum() > 0:
        known_preds = predictions[known_mask]
        known_labels = labels[known_mask]
        known_correct = (known_preds == known_labels).sum()
        known_acc = known_correct / len(known_preds) * 100
        
        print(f"\nâœ… Known Classes (4ì¢… í•´ì¶©):")
        print(f"   - ì •í™•ë„: {known_acc:.2f}% ({known_correct}/{len(known_preds)})")
        
        # í´ë˜ìŠ¤ë³„ ì •í™•ë„
        for class_id in range(4):
            class_mask = known_labels == class_id
            if class_mask.sum() > 0:
                class_acc = (known_preds[class_mask] == class_id).sum() / class_mask.sum() * 100
                print(f"   - {PEST_CLASSES[class_id]}: {class_acc:.2f}%")
    
    # 2. Unknown í´ë˜ìŠ¤ ê±°ë¶€ìœ¨
    unknown_mask = is_known == False
    if unknown_mask.sum() > 0:
        unknown_preds = predictions[unknown_mask]
        unknown_rejected = (unknown_preds == -1).sum()
        rejection_rate = unknown_rejected / len(unknown_preds) * 100
        
        print(f"\nâŒ Unknown Classes (6ì¢… ì¼ë°˜ê³¤ì¶©):")
        print(f"   - ê±°ë¶€ìœ¨: {rejection_rate:.2f}% ({unknown_rejected}/{len(unknown_preds)})")
        print(f"   - ì˜¤ì¸ì‹: {100-rejection_rate:.2f}% ({len(unknown_preds)-unknown_rejected}ê°œ)")
    
    # 3. ì „ì²´ ì„±ëŠ¥
    total_correct = 0
    # Knownì„ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜
    total_correct += ((predictions[known_mask] == labels[known_mask]).sum() if known_mask.sum() > 0 else 0)
    # Unknownì„ ì˜¬ë°”ë¥´ê²Œ ê±°ë¶€
    total_correct += ((predictions[unknown_mask] == -1).sum() if unknown_mask.sum() > 0 else 0)
    
    total_acc = total_correct / len(predictions) * 100
    
    print(f"\nğŸ“ˆ ì „ì²´ Open Set ì„±ëŠ¥:")
    print(f"   - ì •í™•ë„: {total_acc:.2f}% ({total_correct}/{len(predictions)})")
    
    # 4. ìœ ì‚¬ë„ í†µê³„
    print(f"\nğŸ“ ìœ ì‚¬ë„ í†µê³„:")
    print(f"   - Known í‰ê· : {similarities[known_mask].mean():.3f} (Â±{similarities[known_mask].std():.3f})")
    print(f"   - Unknown í‰ê· : {similarities[unknown_mask].mean():.3f} (Â±{similarities[unknown_mask].std():.3f})")
    
    return {
        'known_acc': known_acc if known_mask.sum() > 0 else 0,
        'rejection_rate': rejection_rate if unknown_mask.sum() > 0 else 0,
        'total_acc': total_acc
    }

def main():
    # ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # ë°ì´í„° ë³€í™˜
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ (ìˆ˜ì • í•„ìš”)
    data_root = './insect_dataset'
    
    # Train ë°ì´í„°ì…‹ (4ì¢… í•´ì¶©ë§Œ)
    print("\nğŸ“‚ Train ë°ì´í„° ë¡œë“œ...")
    train_dataset = PestDataset(data_root, transform=transform, mode='train')
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=2)
    
    # ResNet50 íŠ¹ì§• ì¶”ì¶œê¸°
    model = models.resnet50(pretrained=True)
    model = nn.Sequential(*list(model.children())[:-1])  # FC ë ˆì´ì–´ ì œê±°
    model = model.to(device)
    model.eval()
    
    print("\nğŸ” íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...")
    features_dict = extract_features(model, train_loader, device)
    
    print("\nğŸ“Š í”„ë¡œí† íƒ€ì… ê³„ì‚°...")
    prototypes = compute_prototypes(features_dict)
    
    print("\nğŸ“ ì„ê³„ê°’ ê³„ì‚°...")
    thresholds = compute_thresholds(features_dict, prototypes, percentile=5)
    
    # Test ë°ì´í„°ì…‹ (ì „ì²´ 10ì¢…)
    print("\nğŸ“‚ Test ë°ì´í„° ë¡œë“œ...")
    test_dataset = FullDataset(data_root, transform=transform, mode='test')
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)
    
    print("\nğŸ§ª Open Set Recognition í…ŒìŠ¤íŠ¸...")
    predictions, labels, is_known, similarities = test_open_set(
        model, test_loader, prototypes, thresholds, device
    )
    
    # í‰ê°€
    results = evaluate_results(predictions, labels, is_known, similarities)
    
    # ê²°ê³¼ ì €ì¥
    save_data = {
        'prototypes': prototypes,
        'thresholds': thresholds,
        'class_names': PEST_CLASSES,
        'test_results': results
    }
    
    # Pickleë¡œ ì €ì¥
    with open('pest_prototypes.pkl', 'wb') as f:
        pickle.dump(save_data, f)
    
    # JSONìœ¼ë¡œë„ ì €ì¥ (ì½ê¸° í¸í•¨)
    result_json = {
        'thresholds': {k: float(v) for k, v in thresholds.items()},
        'class_names': PEST_CLASSES,
        'test_results': results
    }
    with open('pest_prototypes.json', 'w') as f:
        json.dump(result_json, f, indent=2)
    
    print("\n" + "="*60)
    print("âœ… ì™„ë£Œ!")
    print("- pest_prototypes.pkl: í”„ë¡œí† íƒ€ì… ë²¡í„° ë° ê²°ê³¼")
    print("- pest_prototypes.json: ì„ê³„ê°’ ë° í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)

if __name__ == "__main__":
    main()