"""
Open Set Recognition ì„œë²„ ëª¨ë“ˆ
2ê°œ ì•™ìƒë¸”: ê±°ë¦¬ ê¸°ë°˜ + í™•ë¥  ê¸°ë°˜
ì•Œë¦¼ í­íƒ„ ë°©ì§€ ë° ì˜ìƒ ìš©ëŸ‰ ê´€ë¦¬ í¬í•¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import cv2
import os
import json
from datetime import datetime, timedelta
from pathlib import Path
import asyncio
import logging
from typing import List, Dict, Tuple
import hashlib

class OpenSetRecognizer:
    """Open Set í•´ì¶© ì¸ì‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, known_classes=4, confidence_threshold=0.65):
        """
        Args:
            known_classes: í•™ìŠµëœ í•´ì¶© ì¢…ë¥˜ ìˆ˜ (í˜„ì¬ 4ì¢…)
            confidence_threshold: ìµœì¢… ì‹ ë¢°ë„ ì„ê³„ê°’
        """
        self.known_classes = known_classes
        self.confidence_threshold = confidence_threshold
        
        # ì•Œë ¤ì§„ í•´ì¶© (í•™ìŠµëœ 4ì¢…)
        self.known_insects = {
            0: "ê½ƒë…¸ë‘ì´ì±„ë²Œë ˆ",
            1: "ë‹´ë°°ê°€ë£¨ì´",
            2: "ë³µìˆ­ì•„í˜¹ì§„ë”§ë¬¼",
            3: "ì©ë©ë‚˜ë¬´ë…¸ë¦°ì¬"
        }
        
        # íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë“œ
        self.feature_extractor = self._load_feature_extractor()
        
        # í´ë˜ìŠ¤ë³„ í”„ë¡œí† íƒ€ì… (í•™ìŠµ ë°ì´í„°ì˜ í‰ê·  íŠ¹ì§•)
        self.class_prototypes = self._load_prototypes()
        
        # ì•Œë¦¼ ì¿¨ë‹¤ìš´ ê´€ë¦¬
        self.alert_cooldown = {}
        self.cooldown_minutes = 30
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def _load_feature_extractor(self):
        """íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸ ë¡œë“œ"""
        from torchvision import models
        model = models.resnet50(pretrained=True)
        # ë§ˆì§€ë§‰ FC ë ˆì´ì–´ ì œê±° (íŠ¹ì§•ë§Œ ì¶”ì¶œ)
        model = nn.Sequential(*list(model.children())[:-1])
        model.eval()
        return model
    
    def _load_prototypes(self):
        """ê° í´ë˜ìŠ¤ì˜ í”„ë¡œí† íƒ€ì… íŠ¹ì§• ë¡œë“œ"""
        import pickle
        
        # í•™ìŠµëœ í”„ë¡œí† íƒ€ì… íŒŒì¼ í™•ì¸
        prototype_file = 'pest_prototypes.pkl'
        
        if os.path.exists(prototype_file):
            # í•™ìŠµëœ í”„ë¡œí† íƒ€ì… ë¡œë“œ
            with open(prototype_file, 'rb') as f:
                data = pickle.load(f)
                prototypes = data['prototypes']
                self.thresholds = data.get('thresholds', {})
                self.logger.info(f"âœ… í”„ë¡œí† íƒ€ì… ë¡œë“œ ì™„ë£Œ: {prototype_file}")
                return prototypes
        else:
            # ê²½ê³ : ë”ë¯¸ ê°’ ì‚¬ìš©
            self.logger.warning("âš ï¸ í”„ë¡œí† íƒ€ì… íŒŒì¼ ì—†ìŒ! ë”ë¯¸ ê°’ ì‚¬ìš© (ì •í™•ë„ ë‚®ìŒ)")
            self.logger.warning(f"ğŸ‘‰ ë¨¼ì € ì‹¤í–‰: python train_prototypes.py")
            
            prototypes = {}
            for class_id in self.known_insects.keys():
                prototypes[class_id] = np.random.randn(2048)  # ResNet50 íŠ¹ì§• í¬ê¸°
                prototypes[class_id] = prototypes[class_id] / np.linalg.norm(prototypes[class_id])
            return prototypes
    
    def extract_features(self, image):
        """ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        # ì „ì²˜ë¦¬
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        if isinstance(image, np.ndarray):
            image_tensor = transform(image).unsqueeze(0)
        else:
            image_tensor = image
        
        # íŠ¹ì§• ì¶”ì¶œ
        with torch.no_grad():
            features = self.feature_extractor(image_tensor)
            features = features.squeeze().numpy()
            features = features / np.linalg.norm(features)  # ì •ê·œí™”
        
        return features
    
    def distance_based_verification(self, features):
        """ê±°ë¦¬ ê¸°ë°˜ ê²€ì¦ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)"""
        max_similarity = -1
        best_class = -1
        
        for class_id, prototype in self.class_prototypes.items():
            similarity = cosine_similarity([features], [prototype])[0][0]
            if similarity > max_similarity:
                max_similarity = similarity
                best_class = class_id
        
        # ìœ ì‚¬ë„ë¥¼ í™•ë¥ ë¡œ ë³€í™˜ (0.5 ~ 1.0 ë²”ìœ„)
        confidence = (max_similarity + 1) / 2
        
        return best_class, confidence, max_similarity
    
    def probability_based_verification(self, features):
        """í™•ë¥  ê¸°ë°˜ ê²€ì¦ (ì†Œí”„íŠ¸ë§¥ìŠ¤ with temperature)"""
        temperature = 2.0  # ì˜¨ë„ ë§¤ê°œë³€ìˆ˜ (ë¶ˆí™•ì‹¤ì„± ì¡°ì •)
        
        # ê° í´ë˜ìŠ¤ì™€ì˜ ê±°ë¦¬ ê³„ì‚°
        logits = []
        for class_id, prototype in self.class_prototypes.items():
            similarity = cosine_similarity([features], [prototype])[0][0]
            logits.append(similarity / temperature)
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤
        logits = np.array(logits)
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / exp_logits.sum()
        
        best_class = np.argmax(probs)
        confidence = probs[best_class]
        
        return best_class, confidence, probs
    
    def ensemble_prediction(self, image):
        """ì•™ìƒë¸” ì˜ˆì¸¡ (2ê°œ ë°©ë²• ì¡°í•©)"""
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.extract_features(image)
        
        # 1. ê±°ë¦¬ ê¸°ë°˜
        dist_class, dist_conf, similarity = self.distance_based_verification(features)
        
        # 2. í™•ë¥  ê¸°ë°˜
        prob_class, prob_conf, probs = self.probability_based_verification(features)
        
        # ì•™ìƒë¸” ê²°ì •
        if dist_class == prob_class:
            # ë‘ ë°©ë²•ì´ ì¼ì¹˜
            final_class = dist_class
            final_confidence = (dist_conf + prob_conf) / 2
        else:
            # ë¶ˆì¼ì¹˜ ì‹œ ë” ë†’ì€ ì‹ ë¢°ë„ ì„ íƒ
            if dist_conf > prob_conf:
                final_class = dist_class
                final_confidence = dist_conf * 0.7  # í˜ë„í‹° ì ìš©
            else:
                final_class = prob_class
                final_confidence = prob_conf * 0.7
        
        # Unknown ì²˜ë¦¬
        if final_confidence < self.confidence_threshold:
            return {
                'class_id': -1,
                'class_name': 'ë¯¸í™•ì¸ í•´ì¶©',
                'confidence': final_confidence,
                'is_known': False,
                'details': {
                    'distance_based': {'class': dist_class, 'confidence': dist_conf},
                    'probability_based': {'class': prob_class, 'confidence': prob_conf}
                }
            }
        
        return {
            'class_id': final_class,
            'class_name': self.known_insects.get(final_class, 'ë¯¸í™•ì¸'),
            'confidence': final_confidence,
            'is_known': True,
            'details': {
                'distance_based': {'class': dist_class, 'confidence': dist_conf},
                'probability_based': {'class': prob_class, 'confidence': prob_conf}
            }
        }
    
    def check_alert_cooldown(self, greenhouse_id, insect_type):
        """ì•Œë¦¼ ì¿¨ë‹¤ìš´ ì²´í¬"""
        key = f"{greenhouse_id}_{insect_type}"
        now = datetime.now()
        
        if key in self.alert_cooldown:
            last_alert = self.alert_cooldown[key]
            if now - last_alert < timedelta(minutes=self.cooldown_minutes):
                remaining = self.cooldown_minutes - (now - last_alert).seconds // 60
                self.logger.info(f"ì•Œë¦¼ ì¿¨ë‹¤ìš´ ì¤‘: {insect_type} ({remaining}ë¶„ ë‚¨ìŒ)")
                return False
        
        self.alert_cooldown[key] = now
        return True
    
    def process_video_metadata(self, metadata_path):
        """ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ë° Open Set ë¶„ë¥˜"""
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        greenhouse_id = metadata['greenhouse_id']
        detections = metadata['detections']
        
        # ê° íƒì§€ì— ëŒ€í•´ Open Set ë¶„ë¥˜
        processed_detections = []
        alert_queue = []
        
        for detection in detections:
            crop_path = detection['crop_path']
            
            if os.path.exists(crop_path):
                # ì´ë¯¸ì§€ í’ˆì§ˆ ì²´í¬
                if not self.check_image_quality(crop_path):
                    continue
                
                # Open Set ë¶„ë¥˜
                crop_image = cv2.imread(crop_path)
                result = self.ensemble_prediction(crop_image)
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                detection['open_set_result'] = result
                
                # í•´ì¶©ì´ í™•ì¸ëœ ê²½ìš°
                if result['is_known'] and result['confidence'] > 0.7:
                    insect_type = result['class_name']
                    
                    # ì•Œë¦¼ ì¿¨ë‹¤ìš´ ì²´í¬
                    if self.check_alert_cooldown(greenhouse_id, insect_type):
                        alert_queue.append({
                            'greenhouse_id': greenhouse_id,
                            'insect_type': insect_type,
                            'confidence': result['confidence'],
                            'detection': detection
                        })
                    
                    processed_detections.append(detection)
                else:
                    # í•´ì¶©ì´ ì•„ë‹Œ ê²½ìš° ì´ë¯¸ì§€ ì‚­ì œ
                    os.remove(crop_path)
                    self.logger.info(f"ë¹„í•´ì¶© ì´ë¯¸ì§€ ì‚­ì œ: {crop_path}")
            
        return processed_detections, alert_queue
    
    def check_image_quality(self, image_path, threshold=100):
        """Laplacian varianceë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ í’ˆì§ˆ ì²´í¬"""
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        laplacian = cv2.Laplacian(image, cv2.CV_64F)
        variance = laplacian.var()
        
        return variance > threshold
    
    def create_alert_video(self, original_video, detections, insect_type):
        """íŠ¹ì • í•´ì¶©ë§Œ ë°”ìš´ë”©ë°•ìŠ¤ë¡œ í‘œì‹œí•œ ì˜ìƒ ìƒì„±"""
        cap = cv2.VideoCapture(original_video)
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        output_path = f"alert_{insect_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # í•´ë‹¹ í•´ì¶©ì˜ íƒì§€ë§Œ í•„í„°ë§
        target_detections = [d for d in detections 
                            if d.get('open_set_result', {}).get('class_name') == insect_type]
        
        frame_id = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í˜„ì¬ í”„ë ˆì„ì˜ íƒì§€ ì°¾ê¸°
            frame_detections = [d for d in target_detections if d['frame_id'] == frame_id]
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            for detection in frame_detections:
                bbox = detection['bbox']
                x1, y1, x2, y2 = map(int, bbox)
                
                # ë°•ìŠ¤
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ë¼ë²¨
                label = f"{insect_type} #{detection['track_id']}"
                conf = detection['open_set_result']['confidence']
                cv2.putText(frame, f"{label} ({conf:.2f})", (x1, y1-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            out.write(frame)
            frame_id += 1
        
        cap.release()
        out.release()
        
        return output_path

class VideoStorageManager:
    """ì˜ìƒ ì €ì¥ ìš©ëŸ‰ ê´€ë¦¬"""
    
    def __init__(self, storage_path="./videos", max_days=7, max_size_gb=50):
        self.storage_path = Path(storage_path)
        self.max_days = max_days
        self.max_size_gb = max_size_gb
        self.logger = logging.getLogger(__name__)
    
    def cleanup_old_videos(self):
        """ì˜¤ë˜ëœ ë¹„ë””ì˜¤ ì‚­ì œ"""
        now = datetime.now()
        total_deleted = 0
        
        for video_file in self.storage_path.glob("**/*.mp4"):
            # íŒŒì¼ ìƒì„± ì‹œê°„ í™•ì¸
            file_time = datetime.fromtimestamp(video_file.stat().st_mtime)
            
            if now - file_time > timedelta(days=self.max_days):
                file_size = video_file.stat().st_size
                video_file.unlink()
                total_deleted += file_size
                self.logger.info(f"ì˜¤ë˜ëœ ë¹„ë””ì˜¤ ì‚­ì œ: {video_file.name}")
        
        if total_deleted > 0:
            self.logger.info(f"ì´ {total_deleted / 1024 / 1024:.2f} MB ì‚­ì œ")
    
    def check_storage_usage(self):
        """ì €ì¥ ê³µê°„ ì‚¬ìš©ëŸ‰ ì²´í¬"""
        total_size = sum(f.stat().st_size for f in self.storage_path.glob("**/*") if f.is_file())
        total_gb = total_size / 1024 / 1024 / 1024
        
        if total_gb > self.max_size_gb:
            self.logger.warning(f"ì €ì¥ ê³µê°„ ì´ˆê³¼: {total_gb:.2f} GB / {self.max_size_gb} GB")
            # ê°€ì¥ ì˜¤ë˜ëœ íŒŒì¼ë¶€í„° ì‚­ì œ
            self.cleanup_by_size()
        
        return total_gb
    
    def cleanup_by_size(self):
        """ìš©ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì •ë¦¬"""
        files = sorted(self.storage_path.glob("**/*.mp4"), 
                      key=lambda f: f.stat().st_mtime)
        
        total_size = sum(f.stat().st_size for f in files)
        target_size = self.max_size_gb * 1024 * 1024 * 1024
        
        for file in files:
            if total_size <= target_size:
                break
            
            file_size = file.stat().st_size
            file.unlink()
            total_size -= file_size
            self.logger.info(f"ìš©ëŸ‰ ê´€ë¦¬ ì‚­ì œ: {file.name}")
    
    def compress_video(self, input_path, quality=23):
        """H.264 ì••ì¶•"""
        output_path = input_path.replace(".mp4", "_compressed.mp4")
        
        import subprocess
        cmd = [
            'ffmpeg', '-i', input_path,
            '-c:v', 'libx264', '-crf', str(quality),
            '-preset', 'fast', '-y', output_path
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            
            # ì›ë³¸ ì‚­ì œ ë° êµì²´
            os.remove(input_path)
            os.rename(output_path, input_path)
            
            self.logger.info(f"ë¹„ë””ì˜¤ ì••ì¶• ì™„ë£Œ: {input_path}")
        except subprocess.CalledProcessError as e:
            self.logger.error(f"ì••ì¶• ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # Open Set ì¸ì‹ê¸° ì´ˆê¸°í™”
    recognizer = OpenSetRecognizer(known_classes=4)
    
    # ë¹„ë””ì˜¤ ì €ì¥ ê´€ë¦¬ì
    storage_manager = VideoStorageManager()
    
    # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
    metadata_path = "detection_metadata.json"
    processed, alerts = recognizer.process_video_metadata(metadata_path)
    
    # ì•Œë¦¼ ì˜ìƒ ìƒì„±
    for alert in alerts:
        video_path = recognizer.create_alert_video(
            "original_video.mp4",
            processed,
            alert['insect_type']
        )
        print(f"ì•Œë¦¼ ì˜ìƒ ìƒì„±: {video_path}")
    
    # ì €ì¥ ê³µê°„ ê´€ë¦¬
    storage_manager.cleanup_old_videos()
    usage = storage_manager.check_storage_usage()
    print(f"í˜„ì¬ ì €ì¥ ê³µê°„ ì‚¬ìš©: {usage:.2f} GB")